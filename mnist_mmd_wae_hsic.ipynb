{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math, os\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "z_dim = 10\n",
    "# NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 2135\n",
    "# NUM_EXAMPLES_PER_EPOCH_FOR_TEST = 536\n",
    "TOTAL_EXAMPLES = 6434\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = int(TOTAL_EXAMPLES * .8)\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TEST = TOTAL_EXAMPLES - int(TOTAL_EXAMPLES * .8)\n",
    "# Lambda = 0.005 # kl\n",
    "Lambda1 = 1 # mmd\n",
    "Lambda2 = .002 # HSIC, fist axis\n",
    "Lambda3 = .05 # HSIC, rest of axis\n",
    "prob_to_hsic = 1\n",
    "Sigma = 1\n",
    "steps = 18001\n",
    "K1 =  'IMQ' # for mmd, try IMQ, for IMQ, fix bandwidth\n",
    "# K1 = 'Gaussian'\n",
    "K2 =  'Gaussian' # for hsic, try gausian\n",
    "bandwidth1 = np.sqrt(z_dim) * Sigma # or -1 # IMQ for larger mmd\n",
    "# bandwidth1 = 2. * z_dim * (Sigma ** 2) \n",
    "bandwidth2 = 0\n",
    "\n",
    "\n",
    "# bandwidth = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define some handy network layers\n",
    "def lrelu(x, rate=0.1):\n",
    "    return tf.maximum(tf.minimum(x * rate, 0), x)\n",
    "\n",
    "def conv2d_lrelu(inputs, num_outputs, kernel_size, stride):\n",
    "    conv = tf.contrib.layers.convolution2d(inputs, num_outputs, kernel_size, stride, \n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    conv = tf.contrib.layers.batch_norm(\n",
    "          conv, center=True, scale=True,\n",
    "          epsilon=1e-05, decay=0.9, updates_collections=None, fused=False)\n",
    "    \n",
    "    conv = lrelu(conv)\n",
    "    \n",
    "    return conv\n",
    "\n",
    "def conv2d_t_relu(inputs, num_outputs, kernel_size, stride):\n",
    "    conv = tf.contrib.layers.convolution2d_transpose(inputs, num_outputs, kernel_size, stride,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                     activation_fn=tf.identity)\n",
    "    \n",
    "    conv = tf.contrib.layers.batch_norm(\n",
    "          conv, center=True, scale=True,\n",
    "          epsilon=1e-05, decay=0.9, updates_collections=None, fused=False)\n",
    "    \n",
    "    conv = tf.nn.relu(conv)\n",
    "    return conv\n",
    "\n",
    "def fc_lrelu(inputs, num_outputs):\n",
    "    fc = tf.contrib.layers.fully_connected(inputs, num_outputs,\n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    fc = lrelu(fc)\n",
    "    return fc\n",
    "\n",
    "def fc_relu(inputs, num_outputs):\n",
    "    fc = tf.contrib.layers.fully_connected(inputs, num_outputs,\n",
    "                                           weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           activation_fn=tf.identity)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    return fc\n",
    "\n",
    "def max_pool2d(inputs, kernel_size):\n",
    "    return tf.contrib.layers.max_pool2d(inputs, kernel_size)\n",
    "  \n",
    "def dropout(inputs):\n",
    "    return tf.contrib.layers.dropout(inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# DATASET = \"...\"\n",
    "\n",
    "lidc = np.load(DATASET)\n",
    "\n",
    "lidc_train_x = lidc['train_x']/255.\n",
    "lidc_train_y = lidc['train_y']\n",
    "lidc_test_x = lidc['test_x']/255.\n",
    "lidc_test_y = lidc['test_y']\n",
    "\n",
    "print(lidc_train_x.shape)\n",
    "print(lidc_test_x.shape)\n",
    "\n",
    "unique, counts = np.unique(lidc_train_y, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(lidc_train_x[150], cmap='Greys_r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lidc_encoder(x, z_dim, reuse=False):\n",
    "  with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('encoder') as en:\n",
    "        if reuse:\n",
    "            en.reuse_variables()\n",
    "        \n",
    "        conv1 = conv2d_lrelu(x, 64, 4, 2)\n",
    "        pool1 = max_pool2d(conv1, [2, 2])\n",
    "#         drop1 = dropout(pool1)\n",
    "        \n",
    "        conv2 = conv2d_lrelu(pool1, 128, 4, 2)\n",
    "        pool2 = max_pool2d(conv2, [2, 2])\n",
    "#         drop2 = dropout(pool2)\n",
    "\n",
    "        conv3 = conv2d_lrelu(pool2, 256, 4, 1)\n",
    "    \n",
    "        print(conv3)\n",
    "        flat_z = tf.reshape(conv3, [-1, np.prod(conv3.get_shape().as_list()[1:])])\n",
    "        \n",
    "        fc1 = tf.contrib.layers.fully_connected(flat_z, 256, activation_fn=lrelu)\n",
    "        \n",
    "        return tf.contrib.layers.fully_connected(fc1, z_dim * 2, activation_fn=tf.identity)\n",
    "  \n",
    "def lidc_decoder(z, reuse=False):\n",
    "  with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('decoder') as vs:\n",
    "        if reuse:\n",
    "            vs.reuse_variables()\n",
    "        \n",
    "        fc2 = tf.contrib.layers.fully_connected(z, 256, activation_fn=lrelu)\n",
    "\n",
    "        fc3 = fc_relu(fc2, 3*3*256)\n",
    "        fc3 = tf.reshape(fc3, tf.stack([tf.shape(fc3)[0], 3, 3, 256]))\n",
    "      \n",
    "        deconv1 = conv2d_t_relu(fc3, 128, 4, 2)\n",
    "        deconv2 = conv2d_t_relu(deconv1, 64, 4, 2)\n",
    "        deconv3 = conv2d_t_relu(deconv2, 32, 4, 2)\n",
    "        deconv4 = tf.contrib.layers.convolution2d_transpose(deconv3, 1, 4, 2,\n",
    "                                                     weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                     activation_fn=tf.identity)\n",
    "\n",
    "        return tf.nn.sigmoid(deconv4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_med(X,kernel='Gaussian'):\n",
    "    H = compute_diff(X,X)\n",
    "    H = tf.where(tf.greater(H,0))\n",
    "    if tf.shape(H)[0] == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        h = tf.contrib.distributions.percentile(H, 50.0)\n",
    "        if kernel == 'Gaussian':\n",
    "            return tf.sqrt(tf.cast(h/2,tf.float32)) \n",
    "            #return np.sqrt(0.5 * h) / np.log(X.shape[0]+1)\n",
    "        else:\n",
    "            return tf.sqrt(tf.cast(h,tf.float32))\n",
    "\n",
    "def compute_diff(X,Y):\n",
    "    XY = X[:,tf.newaxis,:] - Y\n",
    "    out = tf.einsum('ijk,ijk->ij',XY,XY)\n",
    "    return out\n",
    "    \n",
    "def compute_gram(X,Y, bandwidth, kernel='Gaussian'):\n",
    "    H = compute_diff(X,Y)\n",
    "    if kernel == 'Gaussian':\n",
    "        return tf.exp(- H / 2 / tf.cast(bandwidth**2, tf.float32)) # (x_size, y_size)\n",
    "    else: # inverse multiquadratics kernel\n",
    "        # return c / (c + H) \n",
    "        return tf.pow(tf.cast(bandwidth**2,tf.float32) + H, -0.5)\n",
    "\n",
    "# MMD\n",
    "\n",
    "def MMD(X, Y, kernel='Gaussian', bandwidth=0):\n",
    "    \n",
    "    if bandwidth <= 0: # median heuristic\n",
    "        bandwidth = comp_med(tf.concat([X,Y],0))\n",
    "\n",
    "    XX = compute_gram(X,X,bandwidth,kernel)\n",
    "    YY = compute_gram(Y,Y,bandwidth,kernel)\n",
    "    XY = compute_gram(X,Y,bandwidth,kernel)\n",
    "    XX = tf.reduce_mean(XX)\n",
    "    YY = tf.reduce_mean(YY)\n",
    "    XY = tf.reduce_mean(XY)\n",
    "    \n",
    "    return XX + YY - 2*XY # biased V-stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mean, logvar, random=True):\n",
    "    if random:\n",
    "        eps = tf.random_normal(shape=tf.shape(mean))\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "    else: # deterministic\n",
    "        return mean\n",
    "    \n",
    "def push_forward(encoder, train_x):\n",
    "    mean, logvar = tf.split(encoder(train_x,z_dim), num_or_size_splits=2, axis=1)\n",
    "    return reparameterize(mean,logvar)\n",
    "    \n",
    "def compute_kl_loss(mean, logvar):\n",
    "        \n",
    "    kl_divergence = 1 + logvar - tf.square(mean) - tf.exp(logvar)\n",
    "    kl_divergence = -0.5 * tf.reduce_sum(kl_divergence, 1)\n",
    "\n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HSIC(X, Y, kernel='Gaussian', bandwidthX=0, bandwidthY=0, normalized=True): # set normalized to False\n",
    "    if tf.keras.backend.ndim(X) == 1:\n",
    "        X = tf.expand_dims(X,axis=1)\n",
    "    if tf.keras.backend.ndim(Y) == 1:\n",
    "        Y = tf.expand_dims(Y,axis=1)\n",
    "    n = tf.shape(X)[0]\n",
    "    \n",
    "    if bandwidthX == 0:\n",
    "        bandwidthX = comp_med(X,kernel)\n",
    "    if bandwidthY == 0:\n",
    "        bandwidthY = comp_med(Y,kernel)\n",
    "        \n",
    "    K = compute_gram(X,X,bandwidthX,kernel)\n",
    "    L = compute_gram(Y,Y,bandwidthY,kernel)\n",
    "    H = tf.eye(n) - (tf.ones([n,n]) / tf.cast(n,tf.float32))\n",
    "    Kc = tf.matmul(tf.matmul(H,K),H);\n",
    "    trace = tf.reduce_sum(L*(tf.transpose(Kc)))\n",
    "    if normalized:\n",
    "        HKH = tf.norm(tf.matmul(tf.matmul(H,K),H),ord='fro',axis=[0,1])\n",
    "        HLH = tf.norm(tf.matmul(tf.matmul(H,L),H),ord='fro',axis=[0,1])\n",
    "        return trace / (HKH*HLH)\n",
    "    else:\n",
    "        return trace / tf.cast((n*n),tf.float32)\n",
    "\n",
    "\n",
    "def gather_cols(params, indices, name=None):\n",
    "    \"\"\"Gather columns of a 2D tensor.\n",
    "\n",
    "    Args:\n",
    "        params: A 2D tensor.\n",
    "        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.\n",
    "        name: A name for the operation (optional).\n",
    "\n",
    "    Returns:\n",
    "        A 2D Tensor. Has the same type as ``params``.\n",
    "    \"\"\"\n",
    "    with tf.op_scope([params, indices], name, \"gather_cols\") as scope:\n",
    "        # Check input\n",
    "        params = tf.convert_to_tensor(params, name=\"params\")\n",
    "        indices = tf.convert_to_tensor(indices, name=\"indices\")\n",
    "        try:\n",
    "            params.get_shape().assert_has_rank(2)\n",
    "        except ValueError:\n",
    "            raise ValueError('\\'params\\' must be 2D.')\n",
    "        try:\n",
    "            indices.get_shape().assert_has_rank(1)\n",
    "        except ValueError:\n",
    "            raise ValueError('\\'params\\' must be 1D.')\n",
    "\n",
    "        # Define op\n",
    "        p_shape = tf.shape(params)\n",
    "        p_flat = tf.reshape(params, [-1])\n",
    "        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],\n",
    "                                       [-1, 1]) + indices, [-1])\n",
    "        return tf.reshape(tf.gather(p_flat, i_flat),\n",
    "                          [p_shape[0], -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the computation graph for training\n",
    "train_x = tf.placeholder(tf.float32, shape=[None, 48, 48, 1])\n",
    "train_y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "\n",
    "# mean = logvar = encoder(train_x, z_dim)\n",
    "# train_z, logvar = tf.split(encoder(train_x, z_dim), num_or_size_splits=2, axis=1)\n",
    "# train_xr = decoder(train_z)\n",
    "\n",
    "# train_z = reparameterize(mean, logvar)\n",
    "\n",
    "train_z = push_forward(lidc_encoder, train_x)\n",
    "train_xr = lidc_decoder(train_z)\n",
    "\n",
    "# Build the computation graph for generating samples# Build \n",
    "gen_z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "gen_x = lidc_decoder(gen_z, reuse=True)\n",
    "print(gen_x)\n",
    "\n",
    "pretrained_mean, pretrained_var = tf.split(lidc_encoder(train_x, z_dim, reuse=True), num_or_size_splits=2, axis=1)\n",
    "# pretrained_z = reparameterize(pretrained_mean, pretrained_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the generated z with true samples from a standard Gaussian, and compute their MMD distance\n",
    "true_samples = tf.random_normal([batch_size, z_dim],stddev=Sigma)\n",
    "loss_mmd = MMD(true_samples, train_z, kernel=K1, bandwidth=bandwidth1)\n",
    "\n",
    "# loss_kl = compute_kl_loss(mean, logvar)\n",
    "# loss_ksd = KSD(train_z, bandwidth=bandwidth)\n",
    "\n",
    "# flat_train_xr = tf.reshape(train_xr, [-1, 48*48])\n",
    "# flat_train_x = tf.reshape(train_x, [-1, 48*48])\n",
    "\n",
    "\n",
    "# loss_nll = 0.05 * tf.reduce_mean(tf.reduce_sum(tf.square(train_xr - train_x), 1))\n",
    "loss_nll = tf.reduce_mean(tf.square(train_xr - train_x))\n",
    "\n",
    "\n",
    "# 50% of the time, set loss_hsic2 = 0 \n",
    "hsic_signal = tf.placeholder(tf.bool)  #placeholder for a single boolean value\n",
    "hsic_trigger = tf.cond(tf.equal(hsic_signal, tf.constant(True)), lambda: tf.constant(1, tf.float32), lambda: tf.constant(0, tf.float32))\n",
    "\n",
    "\n",
    "# apply lambda2 on the first feature axis and discriminate all other axis with lambda3\n",
    "first_axis_hsic =  HSIC(gather_cols(train_z, [0]), train_y, normalized=True)\n",
    "other_axis_hsic =  HSIC(gather_cols(train_z, list(range(1,z_dim))), train_y, normalized=True)\n",
    "\n",
    "loss_hsic = Lambda2 * first_axis_hsic - Lambda3 * other_axis_hsic \n",
    "# loss_hsic = Lambda2 * HSIC(train_z, train_y, normalized=False)\n",
    "\n",
    "loss_hsic = hsic_trigger * loss_hsic\n",
    "\n",
    "# loss = tf.reduce_mean(loss_nll + Lambda * loss_kl)\n",
    "loss = loss_nll + Lambda1 * loss_mmd - loss_hsic\n",
    "# loss = loss_nll + loss_mmd\n",
    "# loss = tf.reduce_sum(loss_nll + Lambda * loss_ksd)\n",
    "\n",
    "\n",
    "trainer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('mnist_data')\n",
    "\n",
    "# Convert a numpy array of shape [batch_size, height, width, 1] into a displayable array \n",
    "# of shape [height*sqrt(batch_size, width*sqrt(batch_size))] by tiling the images\n",
    "def convert_to_display(samples):\n",
    "    cnt, height, width = int(math.floor(math.sqrt(samples.shape[0]))), samples.shape[1], samples.shape[2]\n",
    "    samples = np.transpose(samples, axes=[1, 0, 2, 3])\n",
    "    samples = np.reshape(samples, [height, cnt, cnt, width])\n",
    "    samples = np.transpose(samples, axes=[1, 0, 2, 3])\n",
    "    samples = np.reshape(samples, [height*cnt, width*cnt])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# helper function for getting batches\n",
    "def next_batch(images, labels, index_in_epoch, batch_size):\n",
    "    start = index_in_epoch\n",
    "    # increase the index in epoch by the batch size\n",
    "    index_in_epoch += batch_size\n",
    "    end = index_in_epoch\n",
    "    if index_in_epoch > NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN:\n",
    "      end = -1\n",
    "      index_in_epoch = 0\n",
    "      \n",
    "    return images[start:end], labels[start:end], index_in_epoch\n",
    "  \n",
    "def shuffle_data(images, labels):\n",
    "    perm = np.arange(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN)\n",
    "    np.random.shuffle(perm)\n",
    "    images = images[perm]\n",
    "    labels = labels[perm]\n",
    "    return images, labels\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Start training\n",
    "tf.set_random_seed(100)\n",
    "\n",
    "\n",
    "mmd_list = []\n",
    "o_loss_list = []\n",
    "rec_loss_list = []\n",
    "first_hisc_list = []\n",
    "other_hisc_list = []\n",
    "steps_list = []\n",
    "\n",
    "\n",
    "# using median heuristic bandwidth and HSIC\n",
    "start_time = time.time()\n",
    "index_in_epoch = 0\n",
    "for i in range(steps):\n",
    "    \n",
    "    batch_x, batch_y, index_in_epoch = next_batch(lidc_train_x, lidc_train_y, index_in_epoch, batch_size)\n",
    "    if index_in_epoch == 0:\n",
    "      lidc_train_x, lidc_train_y = shuffle_data(lidc_train_x, lidc_train_y)\n",
    "      batch_x, batch_y, index_in_epoch = next_batch(lidc_train_x, lidc_train_y, index_in_epoch, batch_size)\n",
    "      \n",
    "    batch_x = batch_x.reshape(-1, 48, 48, 1)\n",
    "    use_hsic = False\n",
    "    if np.random.random() <= prob_to_hsic:\n",
    "        use_hsic = True\n",
    "    \n",
    "    _, o_loss, nll, mmd, f_hsic, o_hsic = sess.run([trainer, loss, loss_nll, loss_mmd, first_axis_hsic, other_axis_hsic], feed_dict={train_x: batch_x, train_y: batch_y,\n",
    "                                                                                                     hsic_signal: use_hsic})\n",
    "   \n",
    "    if i % 100 == 0:\n",
    "        print(\"Using hsic:\", use_hsic)\n",
    "        print(\"epoch: {}, Overall loss is {}, recon loss is {}, mmd loss is {}, first hsic is {}, other hsic is {}\".format(\n",
    "            i, o_loss, nll, mmd, f_hsic, o_hsic))\n",
    "#         print(\"epoch: {}, Overall loss is {}, Negative log likelihood is {}, mmd loss is {}\".format(\n",
    "#             i, o_loss, nll, mmd))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"time elapsed: {0:.2f}s\".format(elapsed_time))\n",
    "        start_time = time.time()\n",
    "        # storing data for plot \n",
    "        mmd_list += [mmd]\n",
    "        o_loss_list += [o_loss]\n",
    "        rec_loss_list += [nll]\n",
    "        steps_list += [i]\n",
    "        first_hisc_list += [f_hsic]\n",
    "        other_hisc_list += [o_hsic]\n",
    "        \n",
    "    if i % 1000 == 0:\n",
    "        # feed in test image to get generated mmd loss\n",
    "#         test_x, test_y = mnist.test.next_batch(batch_size)\n",
    "        test_x, test_y = lidc_test_x[:batch_size], lidc_test_y[:batch_size]\n",
    "        test_x = test_x.reshape(-1, 48, 48, 1)\n",
    "#         samples, gen_mmd, my_z, fa, oa= sess.run([gen_x, loss_mmd, pretrained_mean, first_axis_hsic, other_axis_hsic],  feed_dict={gen_z: np.random.normal(size=(64, z_dim)), train_x: test_x, train_y: test_y})\n",
    "        samples, gen_mmd, my_z= sess.run([gen_x, loss_mmd, pretrained_mean],  feed_dict={gen_z: np.random.normal(size=(49, z_dim)), train_x: test_x})\n",
    "        plt.imshow(convert_to_display(samples * 255.), cmap='Greys_r')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "#         print(fa, oa)\n",
    "        print(\"generated mmd loss: {}, my_z: {}\".format(gen_mmd, my_z[0]))\n",
    "\n",
    "# saver.save(sess, \"./hsic_test.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "plt.style.use('default')\n",
    "\n",
    "# plotting learning curve\n",
    "# plt.ylim(0.0, 100.0)\n",
    "\n",
    "aggregated_list = np.array(rec_loss_list) + Lambda1 * np.array(mmd_list)\n",
    "\n",
    "# plt.plot(steps_list, aggregated_list, color='red')\n",
    "\n",
    "# plt.xlabel(\"steps\", fontsize=20)\n",
    "# plt.ylabel(\"recon loss\", fontsize=20)\n",
    "# # plt.title(\"reconstruction loss vs steps\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"recon_loss_new.png\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(steps_list, mmd_list, color='green')\n",
    "\n",
    "# plt.xlabel(\"steps\", fontsize=20)\n",
    "# plt.ylabel(\"mmd loss\", fontsize=20)\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # plt.title(\"mmd loss vs steps\")\n",
    "# plt.savefig(\"mmd_loss_new.png\")\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# t = np.arange(0.01, 10.0, 0.01)\n",
    "# data1 = np.exp(t)\n",
    "# data2 = np.sin(2 * np.pi * t)\n",
    "\n",
    "\n",
    "# color = 'tab:red'\n",
    "# ax1.set_xlabel('time (s)')\n",
    "# ax1.set_ylabel('exp', color=color)\n",
    "# ax1.plot(t, data1, color=color)\n",
    "# ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "# color = 'tab:blue'\n",
    "# ax2.set_ylabel('sin', color=color)  # we already handled the x-label with ax1\n",
    "# ax2.plot(t, data2, color=color)\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(steps_list, first_hisc_list, linewidth=2, color='blue')\n",
    "ax1.plot(steps_list, other_hisc_list, linewidth=2, color='aquamarine')\n",
    "\n",
    "ax1.set_xlabel(\"steps\", fontsize=20)\n",
    "ax1.set_ylabel(\"HSIC\", fontsize=20)\n",
    "ax1.set_ylim(top=1)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.plot(steps_list, aggregated_list, linewidth=2, color='magenta')\n",
    "ax2.set_ylabel('loss', fontsize=20) \n",
    "# ax2.set_ylim()\n",
    "\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='HSIC on dep. axis')\n",
    "aqua_patch = mpatches.Patch(color='aquamarine', label='HSIC on indep. axis')\n",
    "magenta = mpatches.Patch(color='magenta', label='recon_loss+MMD')\n",
    "\n",
    "plt.xlim(0, 18500)\n",
    "ax1.legend(handles=[blue_patch, aqua_patch, magenta], loc=7, fontsize=14)\n",
    "\n",
    "\n",
    "# plt.title(\"hisc on dep. axis vs steps\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"lidc_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(steps_list, other_hisc_list, color='aquamarine')\n",
    "# # adding legends \n",
    "# red_patch = mpatches.Patch(color='red', label='recon loss')\n",
    "# green_patch = mpatches.Patch(color='green', label='mmd loss')\n",
    "# blue_patch = mpatches.Patch(color='blue', label='hsic on dep. axis')\n",
    "# aqua_patch = mpatches.Patch(color='aquamarine', label='hsic on indep. axis')\n",
    "\n",
    "# plt.legend(handles=[red_patch, blue_patch, aqua_patch, green_patch], loc=1)\n",
    "\n",
    "# plt.xlabel(\"steps\")\n",
    "# plt.ylabel(\"loss\")\n",
    "# plt.title(\"reconst, hsic and mmd loss vs steps\")\n",
    "\n",
    "# file name format\n",
    "# Lambda1_Lambda2_Lambda3_ratio_accuracy.png\n",
    "# acc = round(acc, 3)\n",
    "# nonorm = no normalization for hsic\n",
    "# plt.savefig(\"./hsic_data/axis1_{0}_{1}_{2}_{3}_{4:.3f}.png\".format(Lambda1, Lambda2, Lambda3, 1-prob_to_hsic, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_z = sess.run(pretrained_mean,  feed_dict={train_x: lidc_test_x.reshape(-1, 48, 48, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depedent_axis = test_z[:, 0]\n",
    "second_z = test_z[:, 1]\n",
    "\n",
    "from sklearn import decomposition\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "pca = decomposition.PCA(n_components=1)\n",
    "print(test_z[:, 1:].shape)\n",
    "pca.fit(test_z[:, 1:])\n",
    "independent_axis_first_component = pca.transform(test_z[:, 1:])\n",
    "\n",
    "label = lidc_test_y\n",
    "\n",
    "# plt.style.use('default')\n",
    "# plt.grid(False)\n",
    "\n",
    "# label_patches = []\n",
    "\n",
    "# x = np.arange(10)\n",
    "# ys = [i+x+(i*x)**2 for i in range(10)]\n",
    "\n",
    "# colors = cm.viridis(np.linspace(0, 1, len(ys)))\n",
    "\n",
    "# c_i = 1\n",
    "# for i in range(1, 6):\n",
    "# #   color = sns.color_palette('viridis')[i]\n",
    "#   color = colors[c_i]\n",
    "#   c_i += 2\n",
    "#   plt.scatter(depedent_axis[label==i], independent_axis_first_component[label==i], \n",
    "#               c=color , s=5)\n",
    "#   label_patch = mpatches.Patch(label=\"score: {}\".format(i), color=color)\n",
    "#   label_patches.append(label_patch)\n",
    "  \n",
    "# # plt.scatter(depedent_axis, independent_axis_first_component, c=label, s=5, cmap='viridis')\n",
    "\n",
    "# # plt.colorbar()\n",
    "# plt.xlim(-2.5, 2.8)\n",
    "# plt.ylim(-4.1, 4.1)\n",
    "\n",
    "# plt.legend(handles=label_patches, loc='best')\n",
    "\n",
    "# # plt.title(\"HSIC dep. axis vs HSIC indep. axis PCA\")\n",
    "# plt.xlabel(\"HSIC dep. axis\", fontsize=20)\n",
    "# plt.ylabel(\"HSIC indep. axis PCA\", fontsize=20)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"hsicplot_new.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_dep_z = []\n",
    "all_indep_z = []\n",
    "\n",
    "label = np.array(label)\n",
    "\n",
    "de_axis = depedent_axis\n",
    "ind_axis = np.array([i[0] for i in independent_axis_first_component])\n",
    "\n",
    "for i in range(1,6):\n",
    "  all_dep_z.append(de_axis[label==i])\n",
    "  all_indep_z.append(ind_axis[label==i])\n",
    "  \n",
    "\n",
    "all_dep_z = np.array(all_dep_z)\n",
    "all_indep_z = np.array(all_indep_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "def plot_hsic_density(ax, all_z, xlim, file_name, vertial=False):\n",
    "  x = np.arange(10)\n",
    "  ys = [i+x+(i*x)**2 for i in range(10)]\n",
    "\n",
    "  colors = cm.viridis(np.linspace(0, 1, len(ys)))\n",
    "  label_patches = []\n",
    "\n",
    "  c_i = 1\n",
    "  score = 1\n",
    "\n",
    "  for i in range(len(all_z)):\n",
    "    color = colors[c_i]\n",
    "    c_i += 2\n",
    "    z = all_z[i]\n",
    "    sns.kdeplot(z, color=color, vertical=vertial)\n",
    "    label = format(score)\n",
    "    score += 1\n",
    "\n",
    "#   ax.xticks([])\n",
    "#   ax.yticks([])\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  if vertial:\n",
    "    ax.set_ylim(xlim[0], xlim[1])\n",
    "    ax.set_xlim(left=0)\n",
    "  else:\n",
    "    ax.set_xlim(xlim[0], xlim[1])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "def plot_hsic(ax):\n",
    "  label_patches = []\n",
    "\n",
    "  x = np.arange(10)\n",
    "  ys = [i+x+(i*x)**2 for i in range(10)]\n",
    "\n",
    "  colors = cm.viridis(np.linspace(0, 1, len(ys)))\n",
    "\n",
    "  c_i = 1\n",
    "  for i in range(1, 6):\n",
    "  #   color = sns.color_palette('viridis')[i]\n",
    "    color = colors[c_i]\n",
    "    c_i += 2\n",
    "    ax.scatter(depedent_axis[label==i], independent_axis_first_component[label==i], \n",
    "                c=color , s=5)\n",
    "    label_patch = mpatches.Patch(label=\"score: {}\".format(i), color=color)\n",
    "    label_patches.append(label_patch)\n",
    "\n",
    "  # plt.scatter(depedent_axis, independent_axis_first_component, c=label, s=5, cmap='viridis')\n",
    "\n",
    "  # plt.colorbar()\n",
    "  ax.set_xlim(-2.5, 2.8)\n",
    "  ax.set_ylim(-4.2, 4.2)\n",
    "\n",
    "  plt.legend(handles=label_patches, loc='best')\n",
    "\n",
    "  # plt.title(\"HSIC dep. axis vs HSIC indep. axis PCA\")\n",
    "  ax.set_xlabel(\"HSIC dep. axis\", fontsize=13.5)\n",
    "  ax.set_ylabel(\"1st prin. comp. of HSIC indep. axis\", fontsize=13.5)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6)) \n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[3, 1], height_ratios=[1,3]) \n",
    "ax1 = plt.subplot(gs[0])\n",
    "plot_hsic_density(ax1, all_dep_z, [-2.5, 2.8],\"hsicplot_dep_new.png\")\n",
    "ax2 = plt.subplot(gs[3])\n",
    "plot_hsic_density(ax2, all_indep_z, [-4.1, 4.1],\"hsicplot_indep_new.png\", vertial=True)\n",
    "ax3 = plt.subplot(gs[2])\n",
    "plot_hsic(ax3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize = (15,8))\n",
    "# ax1 = fig.add_subplot(1,2,1)\n",
    "# ax2 = fig.add_subplot(1,2,2, gridspec_kw = {'width_ratios':[3, 1]})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hsicplot_new.png\")\n",
    "plt.show()\n",
    "\n",
    "print(np.std(depedent_axis))\n",
    "print(np.mean(depedent_axis))\n",
    "print(np.std(independent_axis_first_component))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_axis = test_z[:,0] + ind_axis\n",
    "\n",
    "# sorted_test_z = test_z[test_z[:,0].argsort()]\n",
    "sorted_pca_index = np.argsort(-ind_axis)\n",
    "\n",
    "a = np.argsort(combined_axis)\n",
    "\n",
    "all_sample_z = []\n",
    "y_a = -3 # y axis, -3 to 3\n",
    "sample_pca = -500\n",
    "while y_a <= 3.5:\n",
    "  \n",
    "  while sample_pca < (y_a - 0.05) or sample_pca > (y_a + 0.05):\n",
    "    rd = np.random.normal(size=(1, z_dim-1))\n",
    "    sample_pca = pca.transform(rd)[0][0]\n",
    "    \n",
    "#   print(sample_pca)  \n",
    "  x_a = -2 # x axis, -1.5 to 1.5\n",
    "  count = 0\n",
    "  while x_a < 2:\n",
    "    x_a += 0.4\n",
    "    new_z = np.concatenate((np.array([x_a]), rd[0]))\n",
    "    all_sample_z.append(new_z)\n",
    "    \n",
    "  y_a += 0.7\n",
    "  \n",
    "\n",
    "\n",
    "all_sample_z = np.array(all_sample_z)\n",
    "print(all_sample_z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sess.run(gen_x,  feed_dict={gen_z: all_sample_z, train_x: test_x})\n",
    "\n",
    "def convert_to_display(samples):\n",
    "    cnt, height, width = int(math.floor(math.sqrt(samples.shape[0]))), samples.shape[1], samples.shape[2]\n",
    "    samples = np.transpose(samples, axes=[1, 0, 2, 3])\n",
    "    samples = np.reshape(samples, [height, cnt, cnt, width])\n",
    "    samples = np.transpose(samples, axes=[1, 0, 2, 3])\n",
    "    samples = np.reshape(samples, [height*cnt, width*cnt])\n",
    "    return samples\n",
    "  \n",
    "\n",
    "fig = plt.figure(figsize=(8, 6)) \n",
    "\n",
    "plt.imshow(convert_to_display(samples[:100] *255.), cmap='Greys_r')\n",
    "plt.xticks(np.arange(0, 500, step=50), [str(round(i,1)) for i in np.arange(-2, 2, step=.4)])\n",
    "plt.yticks(np.arange(0, 500, step=50), reversed([str(round(i,1)) for i in np.arange(-3, 3.7, step=.7)]))\n",
    "\n",
    "\n",
    "plt.grid(False)\n",
    "plt.title(\"10x10 generated samples on HSIC dep. axis\", fontsize=15)\n",
    "plt.ylabel(\"1st prin. comp. HSIC (malig. independent)\", fontsize=16)\n",
    "plt.xlabel(\"HSIC (malig. dependent)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"generated_image_lidc.png\")\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(samples[0].reshape((48, 48)), cmap='Greys_r')\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(samples[1].reshape((48, 48)), cmap='Greys_r')\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(samples[10].reshape((48, 48)), cmap='Greys_r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "flattend_train = np.array([i.flatten() for i in lidc_train_x])\n",
    "flattend_samples = np.array([i.reshape((48, 48)).flatten() for i in samples])\n",
    "\n",
    "print(flattend_train.shape)\n",
    "print(flattend_samples.shape)\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(flattend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "distances, indices = nbrs.kneighbors(flattend_samples)\n",
    "\n",
    "score_matrix = []\n",
    "tmp = []\n",
    "for i in range(len(flattend_samples)):\n",
    "  mean_score = np.mean(lidc_train_y[indices[i]])\n",
    "  if (i % 10 == 0 and i != 0):\n",
    "    score_matrix.append(np.array(tmp))\n",
    "    tmp = []\n",
    "  tmp.append(mean_score)\n",
    "\n",
    "score_matrix = np.array(score_matrix)\n",
    "# print(score_matrix)\n",
    "\n",
    "score_matrix.shape\n",
    "\n",
    "score_mean_list = []\n",
    "score_var_list = []\n",
    "for i in range(0, 10):\n",
    "  score_mean_list.append(round(np.mean([s[i] for s in score_matrix]), 2))\n",
    "  score_var_list.append(round(np.std([s[i] for s in score_matrix]), 2))\n",
    "\n",
    "print(score_mean_list) \n",
    "print(score_var_list)\n",
    "\n",
    "score_mean_list.reverse()\n",
    "x, y = pd.Series(np.arange(-2, 2, step=.4), name=\"HSIC dep. axis\"), pd.Series(score_mean_list, name=\"average malig. score\")\n",
    "sns.regplot(x=x, y=y, color ='blue')\n",
    "\n",
    "# plt.title(\"Avg malig. score of generated samples with 3 NNs\", fontsize=14)\n",
    "plt.xlabel(\"HSIC dep. axis\", fontsize=18)\n",
    "plt.ylabel(\"mean malig. score\", fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sample_score.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
